{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = NerSample.objects.filter(dataset__ner_name='MPR_Nerdpool_8-20_v2').exclude(text=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABBR_BASE = \"https://abbr.acdh.oeaw.ac.at/api/abbreviations/?format=json\"\n",
    "\n",
    "\n",
    "def yield_abbr(ABBR_BASE):\n",
    "    \"\"\" iterator to yield all abbreviations from ABBR_BASE \"\"\"\n",
    "    next = True\n",
    "    url = ABBR_BASE\n",
    "    counter = 0\n",
    "    while next:\n",
    "        response = requests.request(\"GET\", url)\n",
    "        result = response.json()\n",
    "        if result.get('next', False):\n",
    "            url = result.get('next')\n",
    "        else:\n",
    "            next = False\n",
    "        results = result.get('results')\n",
    "        for x in results:\n",
    "            text = x.get('orth')\n",
    "            counter += 1\n",
    "            yield(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('blank:de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = {}\n",
    "for x in yield_abbr(ABBR_BASE):\n",
    "    exceptions[x] = [\n",
    "        {ORTH: x}\n",
    "    ]\n",
    "for key, value in exceptions.items():\n",
    "    nlp.tokenizer.add_special_case(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('./prodigy/abbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_samples = []\n",
    "for x in samples:\n",
    "    orig = x.orig_example\n",
    "    spans = orig['spans']\n",
    "    text = orig['text']\n",
    "    doc = nlp(text)\n",
    "    for y in doc.sents:\n",
    "        item = {\n",
    "            'text': y.text,\n",
    "            'spans': []\n",
    "        }\n",
    "        start, end = y.start_char, y.end_char\n",
    "        start_t = y.start\n",
    "        sent_ents = []\n",
    "        for ent in spans:\n",
    "            if ent['start'] >= start and ent['end'] <= end:\n",
    "                new_start = ent['start'] - start\n",
    "                new_end = ent['end'] - start\n",
    "                t_start = ent['token_start']\n",
    "#                 print(f\"sent start: {y.start}, ent_t_start: {t_start}, ent_t_end: {y.end}\")\n",
    "                t_end = ent['token_end']\n",
    "                new_ent = {\n",
    "                    'label': ent['label'],\n",
    "                    'start': new_start,\n",
    "                    'end': new_end,\n",
    "                    'token_start': t_start - y.start,\n",
    "                    'token_end': t_end - y.start\n",
    "                }\n",
    "                item['spans'].append(new_ent)\n",
    "        my_samples.append(item)\n",
    "#     print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(file_path, lines):\n",
    "    \"\"\"Create a .jsonl file and dump contents.\n",
    "    file_path (unicode / Path): The path to the output file.\n",
    "    lines (list): The JSON-serializable contents of each line.\n",
    "    \"\"\"\n",
    "    data = [ujson.dumps(line, escape_forward_slashes=False) for line in lines]\n",
    "    Path(file_path).open('w', encoding='utf-8').write('\\n'.join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl('mrp_short.jsonl', my_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prodigy db-in mrp_short ../mrp_short.jsonl\n",
    "# python manage.py enrich_samples --settings=nerdpool.settings.pg_local\n",
    "# prodigy train ner mrp_short blank:de -o ./mrp_short__blank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
